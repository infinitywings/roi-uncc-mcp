# Ollama Local Model Configuration Example
# Copy this file to config/demo_config.yaml

demo:
  mode: "ai"
  duration: 120
  trials: 3
  grid_model: "2bus-13bus"

ai:
  # Ollama local model settings
  model: "llama3:70b"
  api_base: "http://localhost:11434/v1"
  api_key: ""  # No API key needed for local Ollama
  
  # Model parameters for Llama-3
  temperature: 0.8
  max_tokens: 3000
  top_p: 0.9
  frequency_penalty: 0.0
  presence_penalty: 0.0
  
  # Request settings (local models may be slower)
  timeout: 120
  retry_attempts: 2
  retry_delay: 5

attack:
  prompt: "Design sophisticated attack strategies targeting power grid control systems and communication protocols"
  max_concurrent_attacks: 3
  attack_timeout: 60
  validation_mode: "strict"
  voltage_limit_min: 0.7
  voltage_limit_max: 1.3
  max_power_injection: 5000000

# Rest of configuration uses defaults from main config